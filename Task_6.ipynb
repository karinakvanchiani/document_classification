{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Task_6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL7rNX59znPz"
      },
      "source": [
        "## Данные\n",
        "\n",
        "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
        "- `news_train.txt` тренировочное множество\n",
        "- `news_test.txt` тренировочное множество\n",
        "\n",
        "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
        "\n",
        "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
        "\n",
        ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxww3xPBznP1"
      },
      "source": [
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import pymorphy2\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWJfH8uhznP2"
      },
      "source": [
        "# Задача\n",
        "\n",
        "1. Обработать данные, получив для каждого текста набор токенов\n",
        "Обработать токены с помощью (один вариант из трех):\n",
        "    - pymorphy2\n",
        "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
        "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QQDhiRyznP3"
      },
      "source": [
        "def preprocessing(text):\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return words\n",
        "\n",
        "\n",
        "def get_data(path):\n",
        "    \n",
        "    morph = pymorphy2.MorphAnalyzer()\n",
        "    data = []\n",
        "    \n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        \n",
        "        for line in f:\n",
        "            category, title, text = line.strip().split('\\t')\n",
        "            \n",
        "            title = preprocessing(title)\n",
        "            title = [morph.parse(word)[0].normal_form for word in title]\n",
        "            \n",
        "            text = [preprocessing(sentence) for sentence in re.split(r'[.!?]', text) if len(sentence) > 10]\n",
        "            text = [[morph.parse(word)[0].normal_form for word in sentence] for sentence in text] \n",
        "            \n",
        "            data.append([category, title, text])\n",
        "\n",
        "    return data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2jtSZJbz4Dk",
        "outputId": "03afad9b-017c-4ae0-d37d-261e90f88add"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqWoFWbhznP3"
      },
      "source": [
        "data_train = get_data('/content/gdrive/My Drive/session/documents/news_train.txt')\n",
        "data_test = get_data('/content/gdrive/My Drive/session/documents/news_test.txt')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9CKpBs2znP3"
      },
      "source": [
        "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACpOQAIHznP4"
      },
      "source": [
        "sentences = [note[1] for note in data_train]\n",
        "sentences.extend([sentence for note in data_train for sentence in note[2]])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaP4h9C-znP4"
      },
      "source": [
        "w2v = Word2Vec(sentences, workers=4)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdyC0x8tznP5",
        "outputId": "cdbe2694-e9d3-4f39-bfdd-3391c80a38ad"
      },
      "source": [
        "w2v.wv.most_similar(positive=['яндекс'])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('google', 0.8647419214248657),\n",
              " ('поисковик', 0.8639674186706543),\n",
              " ('сервис', 0.8248758316040039),\n",
              " ('microsoft', 0.8012833595275879),\n",
              " ('вконтакте', 0.7774312496185303),\n",
              " ('facebook', 0.7768967151641846),\n",
              " ('yahoo', 0.7766843438148499),\n",
              " ('соцсеть', 0.7736133337020874),\n",
              " ('мессенджер', 0.7731637358665466),\n",
              " ('amazon', 0.7580886483192444)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtGyObsFznP6"
      },
      "source": [
        "3. Реализовать алгоритм классификации, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
        "     - SVM\n",
        "     - наивный байесовский классификатор\n",
        "     - логистическая регрессия"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89YSNf5SznP6"
      },
      "source": [
        "train_labels = [note[0] for note in data_train]\n",
        "test_labels = [note[0] for note in data_test]\n",
        "\n",
        "train_data = []\n",
        "for note in data_train:\n",
        "    train_data.append(' '.join(note[1]) + ' ' + ' '.join(note[2][0]))\n",
        "    \n",
        "test_data = []\n",
        "for note in data_test:\n",
        "    test_data.append(' '.join(note[1]) + ' ' + ' '.join(note[2][0]))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZkpAfeJznP7",
        "outputId": "a7827c1c-cf88-43e5-8df2-f7a0a0a27710"
      },
      "source": [
        "print(train_data[3])\n",
        "print(train_labels[3])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "с футболист спартак снять четырехматчевой дисквалификация контрольный дисциплинарный комитет кдк рфс снять дисквалификация с полузащитник московский спартак эйден макгидти\n",
            "sport\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vST2Hl2zznP8"
      },
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "X_train_idf = tfidf.fit_transform(train_data)\n",
        "X_test_idf = tfidf.transform(test_data)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAqfe2ZtznP8",
        "outputId": "43db3356-565d-47eb-d3cb-d9b3ffea6656"
      },
      "source": [
        "params = {'C': [0.05, 0.1, 1], \n",
        "          'multi_class': ['ovr', 'multinomial']}\n",
        "\n",
        "log_reg = GridSearchCV(LogisticRegression(random_state=42), \n",
        "                       params, n_jobs=-1)\n",
        "\n",
        "log_reg.fit(X_train_idf, train_labels)\n",
        "\n",
        "preds = log_reg.predict(X_test_idf)\n",
        "score = round(f1_score(test_labels, preds, average='weighted'), 3)\n",
        "\n",
        "print(f'best params: {log_reg.best_params_}')\n",
        "print(f'score = {score}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best params: {'C': 1, 'multi_class': 'multinomial'}\n",
            "score = 0.808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKTg6hjhznP9",
        "outputId": "1609b40d-d43d-45fe-fc9f-e75085132131"
      },
      "source": [
        "params = {'alpha': [0, 0.5, 1]}\n",
        "\n",
        "naive = GridSearchCV(MultinomialNB(),\n",
        "                     params, n_jobs=-1)\n",
        "\n",
        "naive.fit(X_train_idf, train_labels)\n",
        "\n",
        "preds = naive.predict(X_test_idf)\n",
        "score = round(f1_score(test_labels, preds, average='weighted'), 3)\n",
        "\n",
        "print(f'best params: {naive.best_params_}')\n",
        "print(f'score = {score}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "best params: {'alpha': 0.5}\n",
            "score = 0.773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtHVXBA_znP9"
      },
      "source": [
        "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzFeUGzLznP9",
        "outputId": "b6166207-70be-490c-e50c-1e1bd05b156f"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased', do_lower_case=True)\n",
        "model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased', num_labels=10, \n",
        "                                                      output_attentions=False, output_hidden_states=False)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6zis1CKznP-"
      },
      "source": [
        "def get_tokenized_data(data):\n",
        "    new_data = tokenizer.batch_encode_plus(data, add_special_tokens=True, padding='max_length', \n",
        "                                           truncation=True, max_length=256, return_tensors='pt', \n",
        "                                           return_attention_mask=True)\n",
        "    return new_data\n",
        "\n",
        "\n",
        "train_data = get_tokenized_data(train_data)\n",
        "test_data = get_tokenized_data(test_data)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpfMhxIuznP_"
      },
      "source": [
        "counter = Counter(train_labels)\n",
        "mapping = dict(zip(counter, list(range(10))))\n",
        "\n",
        "train_labels = [mapping[key] for key in train_labels]\n",
        "test_labels = [mapping[key] for key in test_labels]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XL9AA6ZznP_"
      },
      "source": [
        "train_input = train_data['input_ids']\n",
        "test_input = test_data['input_ids']\n",
        "\n",
        "train_masks = train_data['attention_mask']\n",
        "test_masks = test_data['attention_mask']\n",
        "\n",
        "train_labels = torch.LongTensor(train_labels)\n",
        "test_labels = torch.LongTensor(test_labels)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSedpzSHznP_"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_data = TensorDataset(train_input, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_input, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_input)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCWlE1WiznQA"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "n_epochs = 3"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlB-qv5yznQA",
        "outputId": "c1dc3b3e-35ed-47f8-ab5e-d63f27f5b5cc"
      },
      "source": [
        "device = torch.device('cuda')\n",
        "model.cuda()\n",
        "\n",
        "losses = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    mean_loss = 0\n",
        "    model.train()\n",
        "    \n",
        "    for batch in train_dataloader:\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        data = batch[0].to(device)\n",
        "        masks = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        outputs = model(data, attention_mask=masks, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        mean_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    mean_loss = mean_loss / len(train_dataloader)\n",
        "    losses.append(mean_loss)\n",
        "    \n",
        "    print(f'loss {mean_loss}')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss 0.7854018273876547\n",
            "loss 0.3870078748795015\n",
            "loss 0.254989714681435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdd52uvoznQB"
      },
      "source": [
        "model.eval()\r\n",
        "predictions = torch.Tensor().to(dtype=torch.int8)\r\n",
        "\r\n",
        "for batch in test_dataloader:\r\n",
        "        \r\n",
        "    data = batch[0].to(device)\r\n",
        "    masks = batch[1].to(device)\r\n",
        "        \r\n",
        "    with torch.no_grad():\r\n",
        "        outputs = model(data, attention_mask=masks, \r\n",
        "                        output_hidden_states=False, \r\n",
        "                        output_attentions=False)\r\n",
        "        \r\n",
        "    predictions = torch.cat((predictions, outputs.logits.cpu().argmax(axis=1)))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b7Fh1XiAvha",
        "outputId": "6228e850-3b51-4916-9804-479dab718f7e"
      },
      "source": [
        "print(classification_report(test_labels, predictions, target_names=mapping.keys()))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       sport       0.95      0.98      0.97       423\n",
            "     culture       0.92      0.89      0.90       426\n",
            "     science       0.93      0.74      0.83       466\n",
            "       media       0.79      0.88      0.83       403\n",
            "   economics       0.82      0.85      0.84       426\n",
            "        life       0.88      0.79      0.83       415\n",
            "      forces       0.72      0.92      0.81       245\n",
            "      travel       0.68      0.70      0.69        54\n",
            "       style       0.65      0.77      0.70        52\n",
            "    business       0.39      0.33      0.36        90\n",
            "\n",
            "    accuracy                           0.84      3000\n",
            "   macro avg       0.77      0.79      0.78      3000\n",
            "weighted avg       0.85      0.84      0.84      3000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLXslQzuFPzy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}